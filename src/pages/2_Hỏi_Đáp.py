"""
Page 2: H·ªèi ƒë√°p AI (Chatbot RAG)
Giao di·ªán chat fullscreen gi·ªëng c√°c AI chat hi·ªán ƒë·∫°i
"""
import streamlit as st
import sys
import os

# Th√™m path ƒë·ªÉ import modules
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from rag_engine import load_vector_db
from prompts.system_prompt import SYSTEM_PROMPT, build_full_prompt
from config import setup_gemini, get_gemini_client, get_gemini_model_name

# Setup
GROQ_API_KEY = setup_gemini()  # T√™n h√†m gi·ªØ nguy√™n ƒë·ªÉ t∆∞∆°ng th√≠ch
groq_client = get_gemini_client()  # T√™n gi·ªØ nguy√™n nh∆∞ng tr·∫£ v·ªÅ Groq client



# Load vector DB
vector_db = load_vector_db()

# Chat History
if "messages" not in st.session_state:
    st.session_state.messages = []

# === HEADER C·ªê ƒê·ªäNH ===
col1, col2 = st.columns([3, 1])
with col1:
    st.markdown("### üí¨ H·ªèi ƒë√°p v·ªõi Chuy√™n gia AI")
with col2:
    show_thinking = st.toggle("üß† Suy nghƒ©", value=False, help="Xem AI ƒëang l√†m g√¨")

# Hi·ªÉn th·ªã k·∫øt qu·∫£ ch·∫©n ƒëo√°n g·∫ßn nh·∫•t (n·∫øu c√≥)
if 'diagnosis_vi' in st.session_state:
    st.info(f"üìã Ch·∫©n ƒëo√°n g·∫ßn nh·∫•t: **{st.session_state['diagnosis_vi']}**")

st.markdown("---")

# === HI·ªÇN TH·ªä T·∫§T C·∫¢ MESSAGES (scroll t·ª± nhi√™n theo trang) ===
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# === INPUT ·ªû D∆Ø·ªöI C√ôNG ===
prompt = st.chat_input("H·ªèi v·ªÅ b·ªánh s·∫ßu ri√™ng, c√°ch ƒëi·ªÅu tr·ªã...")

if prompt:
    # Th√™m v√† hi·ªÉn th·ªã message user
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # X·ª≠ l√Ω response
    with st.chat_message("assistant"):
        if not GROQ_API_KEY:
            st.warning("‚ö†Ô∏è Vui l√≤ng c·∫•u h√¨nh GROQ_API_KEY trong file .env")
        else:
            thinking_container = st.empty()
            
            # === STEP 1: T√¨m ki·∫øm RAG ===
            if show_thinking:
                with thinking_container.container():
                    st.markdown("üîç **ƒêang t√¨m ki·∫øm trong c∆° s·ªü tri th·ª©c...**")
                    with st.status("Truy v·∫•n RAG Database", expanded=True):
                        st.write("üìö K·∫øt n·ªëi ChromaDB...")
            
            retrieved_block = ""
            retrieved_docs_display = []
            has_relevant_docs = False
            
            # C√°c t·ª´ kh√≥a li√™n quan ƒë·ªÉ x√°c ƒë·ªãnh c·∫ßn RAG
            durian_keywords = ['s·∫ßu ri√™ng', 'durian', 'b·ªánh', 'l√°', 'tr√°i', 'th√¢n', 'r·ªÖ', 'thu·ªëc', 'ph√¢n', 'b√≥n', 
                               'phun', 'tr·ªã', 'ch·ªØa', 'tri·ªáu ch·ª©ng', 'v√†ng', 'th·ªëi', 'n·∫•m', 's√¢u', 
                               'c√¥n tr√πng', 'r·ªáp', 'nh·ªán', 'x√¨ m·ªß', 'n·ª©t', 'ch√°y', 'h√©o', 'chƒÉm s√≥c',
                               't∆∞·ªõi', 'c·∫Øt t·ªâa', 'ra hoa', 'ƒë·∫≠u tr√°i', 'thu ho·∫°ch', 'gi·ªëng', 'ri6', 'monthong', 'th√°i',
                               'c√¢y', 'v∆∞·ªùn', 'nh√† v∆∞·ªùn', 'n√¥ng d√¢n', 'th∆∞∆°ng l√°i', 'musang', 'dona']
            
            query_lower = prompt.lower()
            is_durian_related = any(kw in query_lower for kw in durian_keywords)
            
            try:
                if vector_db and is_durian_related:
                    docs = vector_db.similarity_search(prompt, k=3)
                    if docs:
                        for i, d in enumerate(docs):
                            if len(d.page_content) > 50:
                                retrieved_docs_display.append(f"**[{i+1}]** {d.page_content[:150]}...")
                                has_relevant_docs = True
                        if has_relevant_docs:
                            content_list = [f"[{i+1}] {d.page_content}" for i, d in enumerate(docs)]
                            retrieved_block = "TH√îNG TIN THAM KH·∫¢O T·ª™ T√ÄI LI·ªÜU (RAG):\n" + "\n\n".join(content_list)
            except Exception as e:
                print(f"RAG Error: {e}")
            
            if show_thinking:
                with thinking_container.container():
                    with st.status("Truy v·∫•n RAG Database", expanded=True, state="complete"):
                        if has_relevant_docs:
                            st.write("‚úÖ T√¨m th·∫•y t√†i li·ªáu li√™n quan:")
                            for doc in retrieved_docs_display:
                                st.caption(doc)
                        else:
                            st.write("‚ÑπÔ∏è Kh√¥ng t√¨m th·∫•y t√†i li·ªáu c·ª• th·ªÉ")
            
            # === STEP 2: Chu·∫©n b·ªã context ===
            if show_thinking:
                with thinking_container.container():
                    with st.status("Truy v·∫•n RAG Database", expanded=False, state="complete"):
                        st.write("‚úÖ Ho√†n t·∫•t")
                    with st.status("X√¢y d·ª±ng ng·ªØ c·∫£nh", expanded=True):
                        st.write("üìù Ph√¢n t√≠ch l·ªãch s·ª≠ h·ªôi tho·∫°i...")
            
            diag_context = ""
            if 'diagnosis_vi' in st.session_state:
                diag_context = f"L∆ØU √ù NG·ªÆ C·∫¢NH: Ng∆∞·ªùi d√πng v·ª´a upload ·∫£nh v√† ƒë∆∞·ª£c AI ch·∫©n ƒëo√°n c√¢y b·ªã b·ªánh: {st.session_state['diagnosis_vi']}."

            chat_history_text = ""
            recent_msgs = st.session_state.messages[-6:]
            for msg in recent_msgs:
                role_label = "Ng∆∞·ªùi d√πng" if msg["role"] == "user" else "Durian Doctor"
                chat_history_text += f"{role_label}: {msg['content']}\n"

            full_prompt = build_full_prompt(
                SYSTEM_PROMPT, retrieved_block, diag_context, 
                chat_history_text, prompt
            )
            
            # === STEP 3: G·ªçi Groq LLM ===
            if show_thinking:
                with thinking_container.container():
                    with st.status("Truy v·∫•n RAG Database", expanded=False, state="complete"):
                        st.write("‚úÖ Ho√†n t·∫•t")
                    with st.status("X√¢y d·ª±ng ng·ªØ c·∫£nh", expanded=False, state="complete"):
                        st.write("‚úÖ Ho√†n t·∫•t")
                    with st.status("ü§ñ Groq AI ƒëang suy nghƒ©...", expanded=True):
                        st.write("üí≠ Ph√¢n t√≠ch c√¢u h·ªèi v√† t√†i li·ªáu...")
            
            try:
                # Groq API: S·ª≠ d·ª•ng OpenAI-compatible chat completions
                model_name = get_gemini_model_name()
                response = groq_client.chat.completions.create(
                    model=model_name,
                    messages=[
                        {"role": "user", "content": full_prompt}
                    ],
                    temperature=0.7,
                    max_tokens=2048
                )
                bot_reply = response.choices[0].message.content
            except Exception as e:
                bot_reply = f"‚ö†Ô∏è L·ªói k·∫øt n·ªëi Groq API: {e}"


            
            # === Ho√†n t·∫•t ===
            if show_thinking:
                with thinking_container.container():
                    with st.status("Truy v·∫•n RAG Database", expanded=False, state="complete"):
                        st.write("‚úÖ Ho√†n t·∫•t")
                    with st.status("X√¢y d·ª±ng ng·ªØ c·∫£nh", expanded=False, state="complete"):
                        st.write("‚úÖ Ho√†n t·∫•t")
                    with st.status("ü§ñ Groq AI ƒëang suy nghƒ©...", expanded=False, state="complete"):
                        st.write("‚úÖ ƒê√£ t·∫°o c√¢u tr·∫£ l·ªùi")
                    st.markdown("---")
            else:
                thinking_container.empty()
            
            # Hi·ªÉn th·ªã response
            st.markdown(bot_reply)
            
            # Ch·ªâ hi·ªÉn th·ªã ngu·ªìn t√†i li·ªáu khi c√≥ docs li√™n quan
            if has_relevant_docs and retrieved_docs_display:
                with st.expander("üìö Ngu·ªìn t√†i li·ªáu tham kh·∫£o", expanded=False):
                    for i, doc in enumerate(retrieved_docs_display):
                        st.markdown(doc)
                        if i < len(retrieved_docs_display) - 1:
                            st.divider()
            
            # L∆∞u message
            st.session_state.messages.append({"role": "assistant", "content": bot_reply})
