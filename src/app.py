"""
Durian Doctor - ·ª®ng d·ª•ng AI ch·∫©n ƒëo√°n b·ªánh s·∫ßu ri√™ng
Streamlit App v·ªõi Grad-CAM, RAG (Local ChromaDB) v√† Gemini Chatbot
"""

import streamlit as st
import torch
import torch.nn as nn
from torchvision import models, transforms
from PIL import Image
import numpy as np
from pytorch_grad_cam import GradCAM
from pytorch_grad_cam.utils.image import show_cam_on_image
import google.generativeai as genai
import os
from dotenv import load_dotenv

# --- IMPORT M·ªöI CHO RAG ---
from sentence_transformers import SentenceTransformer
from langchain_community.vectorstores import Chroma

# Load bi·∫øn m√¥i tr∆∞·ªùng t·ª´ file .env
load_dotenv()

# --- C·∫§U H√åNH TRANG ---
st.set_page_config(
    page_title="Durian Doctor", 
    page_icon="üå≥", 
    layout="centered",
    initial_sidebar_state="collapsed"
)

# --- 1. SETUP GEMINI API ---
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY", "")
if GOOGLE_API_KEY:
    genai.configure(api_key=GOOGLE_API_KEY)

# --- 2. C·∫§U H√åNH RAG (LOCAL EMBEDDINGS) ---
class LocalSentenceEmbeddings:
    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):
        self.model = SentenceTransformer(model_name)

    def embed_documents(self, texts):
        embs = self.model.encode(texts, show_progress_bar=False)
        return [emb.tolist() if hasattr(emb, 'tolist') else list(emb) for emb in embs]

    def embed_query(self, text):
        emb = self.model.encode([text], show_progress_bar=False)[0]
        return emb.tolist() if hasattr(emb, 'tolist') else list(emb)

def build_chroma_db_if_missing(kb_path: str, db_path: str):
    """Build Chroma DB t·ª´ file knowledge base n·∫øu ch∆∞a t·ªìn t·∫°i"""
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    
    # ƒê·ªçc file knowledge base
    if not os.path.exists(kb_path):
        print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file knowledge base: {kb_path}")
        return None
    
    with open(kb_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # Chia nh·ªè vƒÉn b·∫£n
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=100,
        separators=["\n\n", "\n", ". ", " "]
    )
    chunks = text_splitter.split_text(content)
    
    if not chunks:
        print("‚ö†Ô∏è Kh√¥ng c√≥ n·ªôi dung ƒë·ªÉ t·∫°o DB")
        return None
    
    print(f"üìö ƒêang t·∫°o Chroma DB v·ªõi {len(chunks)} ƒëo·∫°n vƒÉn b·∫£n...")
    
    # T·∫°o DB m·ªõi
    embedding_function = LocalSentenceEmbeddings()
    db = Chroma.from_texts(
        texts=chunks,
        embedding=embedding_function,
        persist_directory=db_path
    )
    print(f"‚úÖ ƒê√£ t·∫°o Chroma DB t·∫°i: {db_path}")
    return db

@st.cache_resource
def load_vector_db():
    """Load Vector Database, t·ª± ƒë·ªông build n·∫øu ch∆∞a c√≥ (h·ªó tr·ª£ Streamlit Cloud)"""
    try:
        embedding_function = LocalSentenceEmbeddings()
        base_dir = os.path.dirname(__file__)
        kb_path = os.path.join(base_dir, '..', 'knowledge_base', 'durian_diseases.txt')
        
        # Th·ª≠ local path tr∆∞·ªõc, n·∫øu kh√¥ng ghi ƒë∆∞·ª£c th√¨ d√πng /tmp (Streamlit Cloud)
        local_db_path = os.path.join(base_dir, '..', 'knowledge_base', 'chroma_db')
        
        # Ki·ªÉm tra n·∫øu local DB ƒë√£ t·ªìn t·∫°i v√† c√≥ d·ªØ li·ªáu
        if os.path.exists(local_db_path) and os.listdir(local_db_path):
            print("üìÇ Loading existing local Chroma DB...")
            db = Chroma(persist_directory=local_db_path, embedding_function=embedding_function)
            return db
        
        # Tr√™n Streamlit Cloud: d√πng /tmp (writable)
        import tempfile
        cloud_db_path = os.path.join(tempfile.gettempdir(), 'chroma_durian_db')
        
        # N·∫øu ƒë√£ build trong /tmp r·ªìi th√¨ load
        if os.path.exists(cloud_db_path) and os.listdir(cloud_db_path):
            print("üìÇ Loading existing Chroma DB from /tmp...")
            db = Chroma(persist_directory=cloud_db_path, embedding_function=embedding_function)
            return db
        
        # Ch∆∞a c√≥ DB ‚Üí build m·ªõi v√†o /tmp
        print("üîÑ Chroma DB ch∆∞a t·ªìn t·∫°i, ƒëang t·ª± ƒë·ªông t·∫°o trong /tmp...")
        db = build_chroma_db_if_missing(kb_path, cloud_db_path)
        return db
        
    except Exception as e:
        print(f"L·ªói load DB: {e}")
        import traceback
        traceback.print_exc()
        return None

# --- 3. C·∫§U H√åNH CLASS B·ªÜNH (11 L·ªõp) ---
CLASS_NAMES = [
    'anthracnose_disease', 'canker_disease', 'fruit_rot', 'leaf_healthy',
    'mealybug_infestation', 'pink_disease', 'sooty_mold', 'stem_blight',
    'stem_cracking_gummosis', 'thrips_disease', 'yellow_leaf'
]

CLASS_NAMES_VI = {
    'anthracnose_disease': 'B·ªánh th√°n th∆∞ (Anthracnose)',
    'canker_disease': 'B·ªánh lo√©t th√¢n (Canker)',
    'fruit_rot': 'Th·ªëi tr√°i (Fruit Rot)',
    'leaf_healthy': 'L√° kh·ªèe m·∫°nh (Healthy)',
    'mealybug_infestation': 'R·ªáp s√°p (Mealybug)',
    'pink_disease': 'B·ªánh h·ªìng (Pink Disease)',
    'sooty_mold': 'N·∫•m mu·ªôi ƒëen (Sooty Mold)',
    'stem_blight': 'Ch√°y th√¢n (Stem Blight)',
    'stem_cracking_gummosis': 'N·ª©t th√¢n x√¨ m·ªß (Gummosis)',
    'thrips_disease': 'B·ªç trƒ© (Thrips)',
    'yellow_leaf': 'V√†ng l√° (Yellow Leaf)'
}

# --- 4. H√ÄM LOAD MODEL VISION ---
@st.cache_resource
def load_model():
    """Load model MobileNetV2 ƒë√£ train"""
    try:
        model = models.mobilenet_v2(weights=None)
        num_features = model.classifier[1].in_features
        model.classifier = nn.Sequential(
            nn.Dropout(p=0.3),
            nn.Linear(num_features, 512),
            nn.ReLU(),
            nn.Dropout(p=0.2),
            nn.Linear(512, len(CLASS_NAMES))
        )
        
        model_path = os.path.join(os.path.dirname(__file__), '..', 'models', 'best_mobilenet_v2.pth')
        
        if os.path.exists(model_path):
            checkpoint = torch.load(model_path, map_location=torch.device('cpu'))
            if 'model_state_dict' in checkpoint:
                model.load_state_dict(checkpoint['model_state_dict'])
            else:
                model.load_state_dict(checkpoint)
            model.eval()
            return model, True
        else:
            return None, False
    except Exception as e:
        return None, False

# --- 5. H√ÄM D·ª∞ ƒêO√ÅN & GRAD-CAM ---
def predict_and_gradcam(image, model):
    preprocess = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])
    
    input_tensor = preprocess(image).unsqueeze(0)

    with torch.no_grad():
        output = model(input_tensor)
        probabilities = torch.nn.functional.softmax(output[0], dim=0)
        confidence, predicted_idx = torch.max(probabilities, 0)
    
    predicted_label = CLASS_NAMES[predicted_idx.item()]
    
    # Grad-CAM
    target_layers = [model.features[-1]]
    cam = GradCAM(model=model, target_layers=target_layers)
    
    rgb_img = np.float32(image.resize((224, 224))) / 255
    grayscale_cam = cam(input_tensor=input_tensor, targets=None)
    visualization = show_cam_on_image(rgb_img, grayscale_cam[0], use_rgb=True)
    
    top3_prob, top3_idx = torch.topk(probabilities, 3)
    top3 = [(CLASS_NAMES[idx.item()], prob.item()) for idx, prob in zip(top3_idx, top3_prob)]
    
    return predicted_label, confidence.item(), visualization, top3

# --- 6. GIAO DI·ªÜN CH√çNH (UI) ---
def main():
    st.title("üå≥ Durian Doctor AI")
    st.markdown("**H·ªá th·ªëng AI ch·∫©n ƒëo√°n b·ªánh s·∫ßu ri√™ng & T∆∞ v·∫•n ƒëi·ªÅu tr·ªã**")
    st.markdown("---")
    
    # Load t√†i nguy√™n
    model, model_loaded = load_model()
    vector_db = load_vector_db()
    
    if not model_loaded:
        st.warning("‚ö†Ô∏è Ch∆∞a t√¨m th·∫•y file model. Vui l√≤ng train xong model.")
    if vector_db is None:
        st.warning("‚ö†Ô∏è Ch∆∞a t√¨m th·∫•y Database. Chatbot s·∫Ω kh√¥ng d√πng RAG.")

    tab1, tab2 = st.tabs(["üì∑ Ch·∫©n ƒëo√°n b·ªánh", "üí¨ H·ªèi ƒë√°p AI"])
    
    # === TAB 1: CH·∫®N ƒêO√ÅN ===
    with tab1:
        st.subheader("üì∑ T·∫£i ·∫£nh l√™n ƒë·ªÉ ch·∫©n ƒëo√°n")
        option = st.radio("Ngu·ªìn ·∫£nh:", ("üìÅ T·∫£i ·∫£nh", "üì∏ Ch·ª•p ·∫£nh"), horizontal=True)
        
        image = None
        if option == "üì∏ Ch·ª•p ·∫£nh":
            camera_file = st.camera_input("Ch·ª•p ·∫£nh")
            if camera_file: image = Image.open(camera_file).convert('RGB')
        else:
            uploaded_file = st.file_uploader("Ch·ªçn ·∫£nh...", type=["jpg", "png", "jpeg"])
            if uploaded_file: image = Image.open(uploaded_file).convert('RGB')
        
        if image:
            col1, col2 = st.columns(2)
            with col1: st.image(image, caption="·∫¢nh g·ªëc", use_container_width=True)
            
            if st.button("üîç Ch·∫©n ƒëo√°n ngay", type="primary"):
                if model_loaded:
                    with st.spinner('üîÑ ƒêang ph√¢n t√≠ch...'):
                        label, conf, heatmap, top3 = predict_and_gradcam(image, model)
                        
                        with col2: st.image(heatmap, caption="Heatmap v√πng b·ªánh", use_container_width=True)
                        
                        st.markdown("---")
                        st.success(f"üéØ **{CLASS_NAMES_VI.get(label, label)}** (ƒê·ªô tin c·∫≠y: {conf*100:.1f}%)")
                        
                        with st.expander("Xem chi ti·∫øt x√°c su·∫•t"):
                            for n, p in top3: st.write(f"- {CLASS_NAMES_VI.get(n, n)}: {p*100:.1f}%")
                        
                        # L∆∞u tr·∫°ng th√°i ƒë·ªÉ Chatbot bi·∫øt
                        st.session_state['diagnosis_vi'] = CLASS_NAMES_VI.get(label, label)
                else:
                    st.error("Ch∆∞a load ƒë∆∞·ª£c model.")

    # === TAB 2: CHATBOT RAG (FINAL UPDATED) ===
    with tab2:
        st.subheader("üí¨ H·ªèi ƒë√°p v·ªõi Chuy√™n gia AI")
        
        if not GOOGLE_API_KEY:
            st.warning("‚ö†Ô∏è Ch∆∞a c·∫•u h√¨nh API Key.")
        
        # Hi·ªÉn th·ªã k·∫øt qu·∫£ ch·∫©n ƒëo√°n g·∫ßn nh·∫•t
        if 'diagnosis_vi' in st.session_state:
            st.info(f"üìã K·∫øt qu·∫£ ch·∫©n ƒëo√°n g·∫ßn nh·∫•t: **{st.session_state['diagnosis_vi']}**")
        
        # Chat History
        if "messages" not in st.session_state:
            st.session_state.messages = []
        
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])
        
        # Input
        if prompt := st.chat_input("H·ªèi v·ªÅ b·ªánh s·∫ßu ri√™ng, c√°ch ƒëi·ªÅu tr·ªã..."):
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"): st.markdown(prompt)
            
            with st.chat_message("assistant"):
                message_placeholder = st.empty()
                
                if not GOOGLE_API_KEY:
                    bot_reply = "‚ö†Ô∏è Thi·∫øu API Key."
                else:
                    # 1. RAG: T√¨m ki·∫øm trong Vector DB
                    retrieved_block = ""
                    try:
                        if vector_db:
                            docs = vector_db.similarity_search(prompt, k=3)
                            if docs:
                                content_list = [f"[{i+1}] {d.page_content}" for i, d in enumerate(docs)]
                                retrieved_block = "TH√îNG TIN THAM KH·∫¢O T·ª™ T√ÄI LI·ªÜU (RAG):\n" + "\n\n".join(content_list)
                            else:
                                retrieved_block = "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong t√†i li·ªáu."
                    except Exception as e:
                        print(f"RAG Error: {e}")
                    
                    # 2. Context Ch·∫©n ƒëo√°n
                    diag_context = ""
                    if 'diagnosis_vi' in st.session_state:
                        diag_context = f"L∆ØU √ù NG·ªÆ C·∫¢NH: Ng∆∞·ªùi d√πng v·ª´a upload ·∫£nh v√† ƒë∆∞·ª£c AI ch·∫©n ƒëo√°n c√¢y b·ªã b·ªánh: {st.session_state['diagnosis_vi']}."

                    # 3. Chat History Context (T·∫°o tr√≠ nh·ªõ ng·∫Øn h·∫°n)
                    chat_history_text = ""
                    # L·∫•y 6 tin nh·∫Øn g·∫ßn nh·∫•t ƒë·ªÉ l√†m ng·ªØ c·∫£nh (User - Bot - User - Bot...)
                    recent_msgs = st.session_state.messages[-6:]
                    for msg in recent_msgs:
                        role_label = "Ng∆∞·ªùi d√πng" if msg["role"] == "user" else "Durian Doctor"
                        chat_history_text += f"{role_label}: {msg['content']}\n"

                    # 4. System Prompt (C·∫≠p nh·∫≠t quy t·∫Øc nh·ªõ & h·ªèi ng∆∞·ª£c)
                    system_prompt = """
B·∫°n l√† "Durian Doctor" - chuy√™n gia n√¥ng nghi·ªáp h√†ng ƒë·∫ßu v·ªÅ c√¢y s·∫ßu ri√™ng t·∫°i Vi·ªát Nam.

QUY T·∫ÆC C·ªêT L√ïI (B·∫ÆT BU·ªòC):
1. **KI·ªÇM TRA L·ªäCH S·ª¨ CHAT (Context Awareness):** Tr∆∞·ªõc khi h·ªèi l·∫°i ng∆∞·ªùi d√πng, H√ÉY ƒê·ªåC K·ª∏ ph·∫ßn "L·ªäCH S·ª¨ TR√í CHUY·ªÜN" b√™n d∆∞·ªõi. N·∫øu ng∆∞·ªùi d√πng ƒë√£ cung c·∫•p th√¥ng tin (nh∆∞ tu·ªïi c√¢y, gi·ªëng, giai ƒëo·∫°n) ·ªü c√°c c√¢u tr∆∞·ªõc, **TUY·ªÜT ƒê·ªêI KH√îNG H·ªéI L·∫†I**. H√£y t·ª± x√¢u chu·ªói th√¥ng tin ƒë·ªÉ tr·∫£ l·ªùi.
2. **T∆∞ v·∫•n c√≥ t√¢m:** N·∫øu ng∆∞·ªùi d√πng h·ªèi chung chung (VD: "B√≥n ph√¢n g√¨?"), h√£y h·ªèi th√™m 2-3 th√¥ng tin quan tr·ªçng nh·∫•t (Tu·ªïi c√¢y, Giai ƒëo·∫°n sinh tr∆∞·ªüng, T√¨nh tr·∫°ng ƒë·∫•t) ƒë·ªÉ t∆∞ v·∫•n ch√≠nh x√°c.
3. **An to√†n:** Ch·ªâ ƒë∆∞a ra t√™n thu·ªëc/li·ªÅu l∆∞·ª£ng n·∫øu c√≥ trong t√†i li·ªáu. Kh√¥ng b·ªãa s·ªë. Ch·ªâ tr·∫£ l·ªùi v·ªÅ s·∫ßu ri√™ng.

C·∫§U TR√öC TR·∫¢ L·ªúI:
- Ch√†o h·ªèi ng·∫Øn g·ªçn.
- N·∫øu thi·∫øu th√¥ng tin -> H·ªèi l·∫°i.
- N·∫øu ƒë·ªß th√¥ng tin -> ƒê∆∞a ra ph√°c ƒë·ªì chi ti·∫øt (Ph√¢n b√≥n, Thu·ªëc, C√°ch l√†m) d·ª±a tr√™n "TH√îNG TIN THAM KH·∫¢O".
                    """
                    
                    # 5. Build Full Prompt
                    full_prompt = f"""
{system_prompt}

{retrieved_block}

{diag_context}

L·ªäCH S·ª¨ TR√í CHUY·ªÜN (CONTEXT):
{chat_history_text}

NG∆Ø·ªúI D√ôNG H·ªéI (C√ÇU M·ªöI NH·∫§T):
{prompt}
"""
                    
                    # 6. Call Gemini
                    try:
                        model_gemini = genai.GenerativeModel('gemini-2.0-flash')
                        response = model_gemini.generate_content(full_prompt)
                        bot_reply = response.text
                    except Exception as e:
                        bot_reply = f"‚ö†Ô∏è L·ªói k·∫øt n·ªëi Google Gemini: {e}"
                
                message_placeholder.markdown(bot_reply)
                st.session_state.messages.append({"role": "assistant", "content": bot_reply})
    # Footer
    st.markdown("---")
    st.markdown(
        """
        <div style='text-align: center; color: gray;'>
        üå≥ Durian Doctor AI - ƒê·ªì √°n t·ªët nghi·ªáp<br>
        Powered by MobileNetV2 + Grad-CAM + Google Gemini
        </div>
        """, 
        unsafe_allow_html=True
    )
if __name__ == "__main__":
    main()